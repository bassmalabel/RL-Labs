{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b3b8c43",
   "metadata": {},
   "source": [
    "This lab focuses on Markov Decision Process Method to solve Reinforcement Learning problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "945881b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c48464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize all the rewards from the given table\n",
    "rewards = {\n",
    "    ((1, 1), (1, 2)): 1,\n",
    "    ((1, 1), (2, 1)): 2/3,\n",
    "    ((1, 2), (1, 1)): 1/2,\n",
    "    ((1, 2), (1, 3)): 3/2,\n",
    "    ((1, 2), (2, 2)): 2,\n",
    "    ((1, 3), (1, 2)): 1/2,\n",
    "    ((1, 3), (2, 3)): 5/2,\n",
    "    ((2, 1), (1, 1)): 1/3,\n",
    "    ((2, 1), (2, 2)): 4/3,\n",
    "    ((2, 1), (3, 1)): 3/2,\n",
    "    ((2, 2), (1, 2)): 1/4,\n",
    "    ((2, 2), (2, 1)): 1/3,\n",
    "    ((2, 2), (2, 3)): 3/2,\n",
    "    ((2, 2), (3, 2)): 3,\n",
    "    ((2, 3), (1, 3)): 1/4,\n",
    "    ((2, 3), (2, 2)): 1,\n",
    "    ((2, 3), (3, 3)): 7/2,\n",
    "    ((3, 1), (2, 1)): 1/2,\n",
    "    ((3, 1), (3, 2)): 3/2,\n",
    "    ((3, 2), (2, 2)): 4/5,\n",
    "    ((3, 2), (3, 1)): 1,\n",
    "    ((3, 2), (3, 3)): 3,\n",
    "    ((3, 3), (2, 3)): 1/2,\n",
    "    ((3, 3), (3, 2)): 4/5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d88d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the components of the MDP problem\n",
    "#set of actions\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up\n",
    "    1: (1, 0),   # Down\n",
    "    2: (0, -1),  # Left\n",
    "    3: (0, 1)    # Right\n",
    "}\n",
    "#set of states \n",
    "states = [(i, j) for i in range(1, 4) for j in range(1, 4)]\n",
    "#discount rate \n",
    "gamma = 0.7\n",
    "#the rewards function \n",
    "def reward(curr_state, action, old_state):\n",
    "  if curr_state == old_state:\n",
    "    reward = -1\n",
    "  else:\n",
    "    reward = rewards[(curr_state, old_state)]\n",
    "  return reward\n",
    "#the transition function \n",
    "def transition_fun(curr_state, action): \n",
    "    next_state = curr_state + action\n",
    "    if next_state in states: \n",
    "        return next_state\n",
    "    else: \n",
    "        return curr_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06287a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of the first policy pi1\n",
    "pi1= {}\n",
    "for (i, j) in states:\n",
    "  if i != 3 :\n",
    "    proba= [0, 1, 0, 0]\n",
    "  else:\n",
    "    proba= [0, 0, 0, 1]\n",
    "  pi1[(i,j)]= proba\n",
    "\n",
    "#initialization of the first policy pi3\n",
    "pi3 = {}\n",
    "for (i, j) in states:\n",
    "  proba = [1/4, 1/4, 1/4, 1/4]\n",
    "  pi3[(i, j)]= proba\n",
    "\n",
    "#initialization of the first policy pi2\n",
    "pi2 = {}\n",
    "for (i, j) in states:\n",
    "  if (i==2) and (j==2):\n",
    "    proba= [1/4, 1/4, 1/4, 1/4]\n",
    "  elif (i!= 2) and (j!= 2):\n",
    "    if (i==j==1):\n",
    "      proba= [0, 1/2, 0, 1/2]\n",
    "    elif (i==j==3):\n",
    "      proba= [1/2, 0, 1/2, 0]\n",
    "    elif (i==1) and (j==3):\n",
    "      proba= [0, 1/2, 1/2, 0]\n",
    "    elif (i==3) and (j==1):\n",
    "      proba= [1/2, 0, 0, 1/2]\n",
    "  elif ((i==2) and (j!=2)) or ((i!=2) and (j==2)):\n",
    "    if (i==2) and (j==1):\n",
    "      proba = [0, 0, 0, 1]\n",
    "    elif (i==2) and (j==3):\n",
    "      proba = [0, 0, 1, 0]\n",
    "    elif (i==1) and (j==2):\n",
    "      probab = [0, 1, 0, 0]\n",
    "    else:\n",
    "      proba= [1, 0, 0, 0]\n",
    "  pi2[(i, j)]= proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e637be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function that selects the actions based on the selected policy\n",
    "def action_selection (curr_state, pi, actions= actions):\n",
    "  probs = pi[curr_state]\n",
    "  action_keys = list(actions.keys())\n",
    "  # Randomly sample one action according to its probability (this works with all policies)\n",
    "  chosen_action = np.random.choice(action_keys, weights=probs, k=1)\n",
    "  return chosen_action\n",
    "\n",
    "#Define a function that return a trajectory following a single episode\n",
    "def get_episode(pi, nbr_steps):\n",
    "  curr_state = (1, 1)\n",
    "  episode = []\n",
    "  while nbr_steps > 0:\n",
    "    action = action_selection(curr_state, pi)\n",
    "    next_state_ = transition_fun(curr_state, action)\n",
    "    reward_ = reward(next_state_, action, curr_state)\n",
    "    episode.append((curr_state, action, reward_))\n",
    "    curr_state = next_state_\n",
    "    nbr_steps -= 1\n",
    "  return episode\n",
    "\n",
    "def get_returns (episode, gamma, returns):\n",
    "  g=0\n",
    "  #we calculate the returns starting from the last state going back all the way to the first state \n",
    "  for _, sar in reversed(episode):\n",
    "    (s, a, r) = sar\n",
    "    g = r + gamma * g\n",
    "    #the returns is a dict where --> keys : states , values : an array of returns \n",
    "    if s in returns:\n",
    "      returns[s].append(g)\n",
    "    else:\n",
    "      returns[s] = [g]\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c49fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo (): \n",
    "    \n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
